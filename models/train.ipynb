{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6345c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VIDUSHI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016813d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw = f.read()\n",
    "\n",
    "try:\n",
    "    dataset = json.loads(raw)\n",
    "except json.JSONDecodeError:\n",
    "    dataset = []\n",
    "    for line in raw.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            dataset.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            # Skip malformed lines; adjust if you need strict validation\n",
    "            continue\n",
    "\n",
    "# Handle if dataset is a dict with a list inside, or directly a list\n",
    "if isinstance(dataset, dict):\n",
    "    for key, value in dataset.items():\n",
    "        if isinstance(value, list):\n",
    "            dataset = value\n",
    "            break\n",
    "\n",
    "positives = []\n",
    "for x in dataset:\n",
    "    positives.append((x.get(\"question\"), x.get(\"exp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753deab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182822"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7141b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [x[\"exp\"] for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b35df9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy -ves \n",
    "def build_easy_negatives(dataset, gold_key_question=\"question\", gold_key_passage=\"exp\", max_attempts=10):\n",
    "    all_passages = [x.get(gold_key_passage) for x in dataset if x.get(gold_key_passage)]\n",
    "    easy_negatives = []\n",
    "    for item in dataset:\n",
    "        q = item.get(gold_key_question)\n",
    "        gold = item.get(gold_key_passage)\n",
    "        if not q or not gold or not all_passages:\n",
    "            continue\n",
    "        candidate = gold\n",
    "        tries = 0\n",
    "        while candidate == gold and tries < max_attempts:\n",
    "            candidate = random.choice(all_passages)\n",
    "            tries += 1\n",
    "        if candidate != gold:\n",
    "            easy_negatives.append((q, candidate))\n",
    "    return easy_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baedd44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_negatives = build_easy_negatives(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ebd975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hard_negatives(dataset, gold_key_question=\"question\", gold_key_passage=\"exp\", sample_size=1000):\n",
    "    all_passages = [x.get(gold_key_passage) for x in dataset if x.get(gold_key_passage)]\n",
    "    \n",
    "    # Sample to speed up BM25 indexing\n",
    "    if len(all_passages) > sample_size:\n",
    "        sampled_idx = random.sample(range(len(all_passages)), sample_size)\n",
    "        all_passages = [all_passages[i] for i in sampled_idx]\n",
    "    \n",
    "    # Build BM25 index\n",
    "    corpus_tokens = [p.lower().split() for p in all_passages]\n",
    "    bm25 = BM25Okapi(corpus_tokens)\n",
    "    \n",
    "    hard_negatives = []\n",
    "    for item in dataset:\n",
    "        q = item.get(gold_key_question)\n",
    "        gold = item.get(gold_key_passage)\n",
    "        if not q or not gold:\n",
    "            continue\n",
    "        \n",
    "        # BM25 rank\n",
    "        q_tokens = q.lower().split()\n",
    "        scores = bm25.get_scores(q_tokens)\n",
    "        ranked = sorted(zip(all_passages, scores), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Take top non-gold passage\n",
    "        for passage, _ in ranked:\n",
    "            if passage != gold:\n",
    "                hard_negatives.append((q, passage))\n",
    "                break\n",
    "    \n",
    "    return hard_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f496fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_negatives = build_hard_negatives(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5225dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_contradiction(text1, text2):\n",
    "    \"\"\"Check if two passages contradict each other.\n",
    "    Simple heuristic - improve with NLI model for production.\n",
    "    \"\"\"\n",
    "    contradiction_pairs = [\n",
    "        ('safe', 'contraindicated'),\n",
    "        ('effective', 'ineffective'),\n",
    "        ('recommended', 'not recommended'),\n",
    "        ('increases', 'decreases'),\n",
    "        ('approved', 'not approved'),\n",
    "        ('use', 'avoid'),\n",
    "        ('beneficial', 'harmful'),\n",
    "        ('normal', 'abnormal')\n",
    "    ]\n",
    "    \n",
    "    text1_lower = text1.lower()\n",
    "    text2_lower = text2.lower()\n",
    "    \n",
    "    for pos, neg in contradiction_pairs:\n",
    "        if (pos in text1_lower and neg in text2_lower) or \\\n",
    "           (neg in text1_lower and pos in text2_lower):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2819c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_medical_topics(text):\n",
    "    \"\"\"Extract medical entities - simple keyword matching.\n",
    "    TODO: Upgrade to scispacy or BioBERT NER for production.\n",
    "    \"\"\"\n",
    "    medical_keywords = {\n",
    "        'diabetes', 'hypertension', 'aspirin', 'metformin', \n",
    "        'surgery', 'pregnancy', 'cancer', 'antibiotics',\n",
    "        'heart', 'blood pressure', 'cholesterol', 'infection',\n",
    "        'pain', 'fever', 'asthma', 'copd', 'stroke', 'mi',\n",
    "        'myocardial infarction', 'coronary', 'cardiovascular'\n",
    "    }\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    return [kw for kw in medical_keywords if kw in text_lower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f08999ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 21 topics for contradiction detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building contradiction negatives: 100%|██████████| 182822/182822 [00:21<00:00, 8576.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 39596 contradiction negatives\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build contradiction negatives (FAST - with hard limits)\n",
    "def build_contradiction_negatives(dataset, max_candidates_per_topic=50):    \n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Step 1: Index passages by medical topics - O(n)\n",
    "    topic_to_passages = defaultdict(list)\n",
    "    \n",
    "    for item in dataset:\n",
    "        passage = item.get(\"exp\")\n",
    "        if not passage:\n",
    "            continue\n",
    "        \n",
    "        topics = extract_medical_topics(passage)\n",
    "        for topic in topics:\n",
    "            topic_to_passages[topic].append(passage)\n",
    "    \n",
    "    print(f\"Indexed {len(topic_to_passages)} topics for contradiction detection\")\n",
    "    \n",
    "    # Step 2: For each question, check LIMITED candidates per topic\n",
    "    contradiction_negatives = []\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"Building contradiction negatives\"):\n",
    "        q = item.get(\"question\")\n",
    "        gold = item.get(\"exp\")\n",
    "        \n",
    "        if not q or not gold:\n",
    "            continue\n",
    "        \n",
    "        # Get topics from gold passage\n",
    "        gold_topics = extract_medical_topics(gold)\n",
    "        if not gold_topics:\n",
    "            continue\n",
    "        \n",
    "        # LIMIT: Only check first max_candidates_per_topic passages per topic\n",
    "        candidates_to_check = []\n",
    "        for topic in gold_topics:\n",
    "            topic_passages = topic_to_passages[topic]\n",
    "            # Sample randomly if too many\n",
    "            if len(topic_passages) > max_candidates_per_topic:\n",
    "                sampled = random.sample(topic_passages, max_candidates_per_topic)\n",
    "                candidates_to_check.extend(sampled)\n",
    "            else:\n",
    "                candidates_to_check.extend(topic_passages)\n",
    "        \n",
    "        # Remove duplicates and gold passage\n",
    "        candidates_to_check = list(set(candidates_to_check))\n",
    "        candidates_to_check = [p for p in candidates_to_check if p != gold and p]\n",
    "        \n",
    "        # Check for contradiction (max 100 candidates per question)\n",
    "        for candidate_passage in candidates_to_check[:100]:\n",
    "            if has_contradiction(gold, candidate_passage):\n",
    "                contradiction_negatives.append((q, candidate_passage))\n",
    "                break  # Found one, move to next question\n",
    "    \n",
    "    print(f\"Built {len(contradiction_negatives)} contradiction negatives\")\n",
    "    return contradiction_negatives\n",
    "\n",
    "contradiction_negatives = build_contradiction_negatives(dataset, max_candidates_per_topic=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aafa9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped passages into 21 medical topics\n",
      "Built 35069 medical hard negatives\n"
     ]
    }
   ],
   "source": [
    "# Build medical hard negatives (most valuable for medical domain!)\n",
    "def build_medical_hard_negatives(dataset, sample_size=1000):    \n",
    "    # Group passages by medical topic/specialty\n",
    "    topic_groups = defaultdict(list)\n",
    "    for item in dataset:\n",
    "        question = item.get(\"question\", \"\")\n",
    "        passage = item.get(\"exp\", \"\")\n",
    "        \n",
    "        if not passage:\n",
    "            continue\n",
    "        \n",
    "        # Extract medical topics from question\n",
    "        topics = extract_medical_topics(question)\n",
    "        \n",
    "        for topic in topics:\n",
    "            topic_groups[topic].append(passage)\n",
    "    \n",
    "    print(f\"Grouped passages into {len(topic_groups)} medical topics\")\n",
    "    \n",
    "    medical_hard_negatives = []\n",
    "    for item in dataset:\n",
    "        q = item.get(\"question\")\n",
    "        gold = item.get(\"exp\")\n",
    "        if not q or not gold:\n",
    "            continue\n",
    "        \n",
    "        topics = extract_medical_topics(q)\n",
    "        \n",
    "        # Sample from SAME medical topic but different passage\n",
    "        for topic in topics:\n",
    "            candidates = [p for p in topic_groups.get(topic, []) if p != gold]\n",
    "            if candidates:\n",
    "                hard_neg = random.choice(candidates)\n",
    "                medical_hard_negatives.append((q, hard_neg))\n",
    "                break\n",
    "    \n",
    "    print(f\"Built {len(medical_hard_negatives)} medical hard negatives\")\n",
    "    return medical_hard_negatives\n",
    "\n",
    "medical_hard_negatives = build_medical_hard_negatives(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fac616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of weighted negatives:\n",
      "  Easy: 160869 → 160869\n",
      "  Hard (BM25): 160869 → 321738\n",
      "  Medical Hard: 35069 → 105207\n",
      "  Contradictions: 39596 → 79192\n",
      "  Total weighted: 667006\n"
     ]
    }
   ],
   "source": [
    "# Combine negatives with strategic weighting\n",
    "all_negatives = (\n",
    "    easy_negatives * 1 +  # 1x easy (baseline)\n",
    "    hard_negatives * 2 +  # 2x hard (BM25 challenging)\n",
    "    medical_hard_negatives * 3 +  # 3x medical hard (domain-specific!)\n",
    "    contradiction_negatives * 2  # 2x contradictions (hallucination prevention!)\n",
    ")\n",
    "\n",
    "print(f\"Summary of weighted negatives:\")\n",
    "print(f\"  Easy: {len(easy_negatives)} → {len(easy_negatives) * 1}\")\n",
    "print(f\"  Hard (BM25): {len(hard_negatives)} → {len(hard_negatives) * 2}\")\n",
    "print(f\"  Medical Hard: {len(medical_hard_negatives)} → {len(medical_hard_negatives) * 3}\")\n",
    "print(f\"  Contradictions: {len(contradiction_negatives)} → {len(contradiction_negatives) * 2}\")\n",
    "print(f\"  Total weighted: {len(all_negatives)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3615aa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built negative index for 160869 unique queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating triplets: 100%|██████████| 160869/160869 [00:00<00:00, 626474.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 160869 triplet samples (query, positive, negative)\n"
     ]
    }
   ],
   "source": [
    "# (query, positive_passage, negative_passage)\n",
    "positives_clean = [(q, p) for (q, p) in positives if q and p]\n",
    "\n",
    "# Use all weighted negatives\n",
    "negatives = all_negatives\n",
    "negatives_clean = [(q, p) for (q, p) in negatives if q and p]\n",
    "\n",
    "# Build a dict mapping query -> list of negative passages\n",
    "from collections import defaultdict\n",
    "\n",
    "neg_by_query = defaultdict(list)\n",
    "for q, p in negatives_clean:\n",
    "    neg_by_query[q].append(p)\n",
    "\n",
    "print(f\"Built negative index for {len(neg_by_query)} unique queries\")\n",
    "\n",
    "# samples: (query, positive_passage, negative_passage)\n",
    "triplet_samples = []\n",
    "for q, pos_p in tqdm(positives_clean, desc=\"Creating triplets\"):\n",
    "    # Fast lookup: get negatives for this query\n",
    "    neg_for_q = neg_by_query.get(q, [])\n",
    "    \n",
    "    # Filter out the positive passage\n",
    "    neg_for_q = [p for p in neg_for_q if p != pos_p]\n",
    "    \n",
    "    if neg_for_q:\n",
    "        neg_p = random.choice(neg_for_q)\n",
    "        triplet_samples.append((q, pos_p, neg_p))\n",
    "\n",
    "random.shuffle(triplet_samples)\n",
    "print(f\"Created {len(triplet_samples)} triplet samples (query, positive, negative)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68bfbbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: What is diabetes?\n",
      "Augmented versions:\n",
      "  1. What is dm?\n",
      "  2. What is diabetes?\n"
     ]
    }
   ],
   "source": [
    "class DataAugmentation:    \n",
    "    @staticmethod\n",
    "    def augment_medical_query(query, num_augments=2):\n",
    "        \"\"\"Augment queries with medical synonyms and paraphrasing.\"\"\"\n",
    "        augmented = [query]\n",
    "        \n",
    "        # Medical synonym replacement\n",
    "        medical_synonyms = {\n",
    "            'heart attack': ['myocardial infarction', 'MI', 'cardiac event'],\n",
    "            'high blood pressure': ['hypertension', 'elevated BP'],\n",
    "            'diabetes': ['diabetes mellitus', 'DM'],\n",
    "            'medicine': ['medication', 'drug', 'pharmaceutical'],\n",
    "            'doctor': ['physician', 'clinician'],\n",
    "            'symptoms': ['signs', 'manifestations'],\n",
    "            'treatment': ['therapy', 'management'],\n",
    "            'side effects': ['adverse effects', 'complications'],\n",
    "        }\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        for original, synonyms in medical_synonyms.items():\n",
    "            if original in query_lower:\n",
    "                for syn in synonyms[:num_augments]:\n",
    "                    new_query = query_lower.replace(original, syn)\n",
    "                    if new_query != query_lower:\n",
    "                        augmented.append(new_query.capitalize())\n",
    "        \n",
    "        # Question reformulation\n",
    "        reformulations = {\n",
    "            'What is': ['What are', 'Can you explain', 'Define'],\n",
    "            'How does': ['How do', 'What is the mechanism of'],\n",
    "            'What are': ['What is', 'List the'],\n",
    "        }\n",
    "        \n",
    "        for original, alternatives in reformulations.items():\n",
    "            if original in query:\n",
    "                for alt in alternatives[:num_augments]:\n",
    "                    new_query = query.replace(original, alt, 1)\n",
    "                    if new_query != query:\n",
    "                        augmented.append(new_query)\n",
    "        \n",
    "        return list(set(augmented))[:num_augments + 1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def augment_passage(passage, drop_rate=0.1):\n",
    "        \"\"\"Augment passages by randomly dropping words (noise injection).\"\"\"\n",
    "        words = passage.split()\n",
    "        num_keep = int(len(words) * (1 - drop_rate))\n",
    "        \n",
    "        if num_keep < len(words) and num_keep > 0:\n",
    "            indices = random.sample(range(len(words)), num_keep)\n",
    "            indices.sort()\n",
    "            return ' '.join([words[i] for i in indices])\n",
    "        \n",
    "        return passage\n",
    "\n",
    "# Test augmentation\n",
    "sample_query = \"What is diabetes?\"\n",
    "augmented = DataAugmentation.augment_medical_query(sample_query)\n",
    "print(f\"Original: {sample_query}\")\n",
    "print(f\"Augmented versions:\")\n",
    "for i, aug in enumerate(augmented[1:], 1):\n",
    "    print(f\"  {i}. {aug}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "debf38c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 160869 original triplets (set USE_AUGMENTATION=True for 3x more)\n"
     ]
    }
   ],
   "source": [
    "# Create triplets WITH augmentation (optional - set to True to use)\n",
    "USE_AUGMENTATION = False  # Set to True for 3x more data\n",
    "\n",
    "if USE_AUGMENTATION:\n",
    "    print(\"Creating augmented triplets...\")\n",
    "    triplet_samples_aug = []\n",
    "    \n",
    "    for q, pos_p in tqdm(positives_clean, desc=\"Creating augmented triplets\"):\n",
    "        neg_for_q = neg_by_query.get(q, [])\n",
    "        neg_for_q = [p for p in neg_for_q if p != pos_p]\n",
    "        \n",
    "        if not neg_for_q:\n",
    "            continue\n",
    "        \n",
    "        # Original triplet\n",
    "        neg_p = random.choice(neg_for_q)\n",
    "        triplet_samples_aug.append((q, pos_p, neg_p))\n",
    "        \n",
    "        # Augmented versions\n",
    "        aug_queries = DataAugmentation.augment_medical_query(q, num_augments=2)\n",
    "        for aug_q in aug_queries[1:]:  # Skip first (original)\n",
    "            aug_pos = DataAugmentation.augment_passage(pos_p)\n",
    "            neg_p = random.choice(neg_for_q)\n",
    "            triplet_samples_aug.append((aug_q, aug_pos, neg_p))\n",
    "    \n",
    "    random.shuffle(triplet_samples_aug)\n",
    "    triplet_samples = triplet_samples_aug\n",
    "    print(f\"Created {len(triplet_samples)} augmented triplet samples\")\n",
    "else:\n",
    "    print(f\"Using {len(triplet_samples)} original triplets (set USE_AUGMENTATION=True for 3x more)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c55240c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset - train: 144782, val: 16087\n"
     ]
    }
   ],
   "source": [
    "# 90-10 train-val split\n",
    "split = int(0.9 * len(triplet_samples))\n",
    "train_data, val_data = triplet_samples[:split], triplet_samples[split:]\n",
    "\n",
    "'''\n",
    "# Use subset for faster iteration (10% of data) during prototyping\n",
    "size = len(train_data) \n",
    "train_data_subset = train_data[:size//10]\n",
    "val_data_subset = val_data[:len(val_data)//10]\n",
    "\n",
    "print(f\"Subset (10%) - train: {len(train_data_subset)}, val: {len(val_data_subset)}\")\n",
    "print(\"Use train_data_subset for fast prototyping, train_data for full training\")\n",
    "'''\n",
    "print(f\"Full dataset - train: {len(train_data)}, val: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73cff91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def collate_batch_triplet(batch, max_len=128):\n",
    "    qs, pos_ps, neg_ps = zip(*batch)\n",
    "    \n",
    "    # Encode queries\n",
    "    enc_q = tokenizer(\n",
    "        list(qs),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Encode positive passages\n",
    "    enc_pos = tokenizer(\n",
    "        list(pos_ps),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Encode negative passages\n",
    "    enc_neg = tokenizer(\n",
    "        list(neg_ps),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return enc_q, enc_pos, enc_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9279c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingScorer(nn.Module):\n",
    "    def __init__(self, encoder, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.hidden_dim = encoder.config.hidden_size\n",
    "        \n",
    "        # Project to embedding space for ranking\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, enc_ips):\n",
    "        out = self.encoder(**enc_ips).last_hidden_state\n",
    "        \n",
    "        # Mean pooling over tokens\n",
    "        mask = enc_ips[\"attention_mask\"].unsqueeze(-1)  # [B, L, 1]\n",
    "        pooled = (out * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)  # [B, H]\n",
    "        \n",
    "        # Project to embedding space\n",
    "        embedding = self.projection(pooled)  # [B, embedding_dim]\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80522ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = RankingScorer(encoder, embedding_dim=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb0aac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# Ranking loss: Triplet Loss with margin\n",
    "def ranking_loss(emb_pos, emb_neg, margin=0.5):\n",
    "    \"\"\"Triplet loss: max(0, margin - (pos_sim - neg_sim))\n",
    "    Push negative embeddings away from positive embeddings.\n",
    "    \"\"\"\n",
    "    # Cosine similarity\n",
    "    pos_sim = F.cosine_similarity(emb_pos, emb_pos)  # [B], should be ~1.0\n",
    "    neg_sim = F.cosine_similarity(emb_pos, emb_neg)  # [B], should be < pos_sim\n",
    "    \n",
    "    # Triplet loss: max(0, margin - (pos_sim - neg_sim))\n",
    "    # Want pos_sim > neg_sim by at least margin\n",
    "    loss = torch.clamp(margin - (pos_sim - neg_sim), min=0).mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "352eb430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches per epoch: 4525\n"
     ]
    }
   ],
   "source": [
    "# Data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_batch_triplet)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, collate_fn=collate_batch_triplet)\n",
    "\n",
    "print(f\"Training batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11b80cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Batch Size: 16 (effective: 64)\n",
      "✓ Max Length: 96 tokens\n",
      "✓ Workers: 0 (single-process for Windows stability)\n",
      "✓ Freeze Encoder: First 2 epochs\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16  # Reduced for 4-6GB VRAM\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Simulate batch_size=64\n",
    "NUM_WORKERS = 0  # ⚠️ Set to 0 on Windows (fixes multiprocessing crash)\n",
    "PIN_MEMORY = True  # Faster CPU->GPU transfer\n",
    "FREEZE_ENCODER_EPOCHS = 2  # Train only projection layer first (faster!)\n",
    "MAX_LENGTH = 96  # Reduce from 128 to save memory\n",
    "\n",
    "# Learning rate schedule\n",
    "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau\n",
    "\n",
    "print(f\"✓ Batch Size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})\")\n",
    "print(f\"✓ Max Length: {MAX_LENGTH} tokens\")\n",
    "print(f\"✓ Workers: {NUM_WORKERS} (single-process for Windows stability)\")\n",
    "print(f\"✓ Freeze Encoder: First {FREEZE_ENCODER_EPOCHS} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61c26337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training batches per epoch: 9049\n",
      "✓ Validation batches: 503\n",
      "✓ Total training samples: 144782\n",
      "✓ Single-process data loading (Windows compatible)\n"
     ]
    }
   ],
   "source": [
    "# Optimized collate function with reduced max length\n",
    "def collate_batch_optimized(batch, max_len=MAX_LENGTH):\n",
    "    qs, pos_ps, neg_ps = zip(*batch)\n",
    "    \n",
    "    # Encode with shorter max_length to save memory\n",
    "    enc_q = tokenizer(\n",
    "        list(qs),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    enc_pos = tokenizer(\n",
    "        list(pos_ps),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    enc_neg = tokenizer(\n",
    "        list(neg_ps),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return enc_q, enc_pos, enc_neg\n",
    "\n",
    "# Optimized data loaders - single process (Windows compatible)\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch_optimized,\n",
    "    num_workers=NUM_WORKERS,  # 0 for Windows\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE * 2,  # Larger batch for validation (no gradients)\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch_optimized,\n",
    "    num_workers=NUM_WORKERS,  # 0 for Windows\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "print(f\"✓ Training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"✓ Validation batches: {len(val_loader)}\")\n",
    "print(f\"✓ Total training samples: {len(train_data)}\")\n",
    "print(f\"✓ Single-process data loading (Windows compatible)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6666addd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting efficient training:\n",
      "  Total steps: 11311\n",
      "  Gradient accumulation: every 4 batches\n",
      "  Early stopping patience: 2 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "EARLY_STOP_PATIENCE = 2  # Stop if no improvement\n",
    "USE_MNR = True\n",
    "# Optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler - warmup + cosine decay\n",
    "total_steps = len(train_loader) * EPOCHS // GRADIENT_ACCUMULATION_STEPS\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=5e-5,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.1,  # 10% warmup\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# Tracking\n",
    "best_val_acc = 0\n",
    "no_improve_count = 0\n",
    "\n",
    "print(f\"Starting efficient training:\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Gradient accumulation: every {GRADIENT_ACCUMULATION_STEPS} batches\")\n",
    "print(f\"  Early stopping patience: {EARLY_STOP_PATIENCE} epochs\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73c17d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Encoder FROZEN (training projection only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 [MNR]:   0%|          | 0/9049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 [MNR]:   0%|          | 3/9049 [00:01<59:41,  2.53it/s, loss=2.9417, lr=2.00e-06]  c:\\Users\\VIDUSHI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoch 0 [MNR]: 100%|██████████| 9049/9049 [10:53<00:00, 13.85it/s, loss=0.8143, lr=4.85e-05]\n",
      "Validation: 100%|██████████| 503/503 [00:58<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 0 Summary:\n",
      "   Train Loss: 1.4934\n",
      "   Val Accuracy: 0.826\n",
      "   Val MRR: 0.913\n",
      "   ✓ New best model saved! (acc=0.826)\n",
      "\n",
      "Epoch 1: Encoder FROZEN (training projection only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [MNR]: 100%|██████████| 9049/9049 [17:54<00:00,  8.42it/s, loss=0.5602, lr=3.75e-05]  \n",
      "Validation: 100%|██████████| 503/503 [01:16<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 Summary:\n",
      "   Train Loss: 1.0127\n",
      "   Val Accuracy: 0.838\n",
      "   Val MRR: 0.919\n",
      "   ✓ New best model saved! (acc=0.838)\n",
      "\n",
      "Epoch 2: Encoder UNFROZEN (full training)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [MNR]: 100%|██████████| 9049/9049 [33:23<00:00,  4.52it/s, loss=0.3618, lr=2.07e-05]\n",
      "Validation: 100%|██████████| 503/503 [01:28<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 2 Summary:\n",
      "   Train Loss: 0.5051\n",
      "   Val Accuracy: 0.942\n",
      "   Val MRR: 0.971\n",
      "   ✓ New best model saved! (acc=0.942)\n",
      "\n",
      "Epoch 3: Encoder UNFROZEN (full training)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [MNR]: 100%|██████████| 9049/9049 [53:58<00:00,  2.79it/s, loss=0.1594, lr=5.85e-06]     \n",
      "Validation: 100%|██████████| 503/503 [01:44<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 3 Summary:\n",
      "   Train Loss: 0.2245\n",
      "   Val Accuracy: 0.950\n",
      "   Val MRR: 0.975\n",
      "   ✓ New best model saved! (acc=0.950)\n",
      "\n",
      "Epoch 4: Encoder UNFROZEN (full training)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [MNR]: 100%|██████████| 9049/9049 [35:19<00:00,  4.27it/s, loss=0.2397, lr=2.00e-10]\n",
      "Validation: 100%|██████████| 503/503 [01:20<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 4 Summary:\n",
      "   Train Loss: 0.1465\n",
      "   Val Accuracy: 0.951\n",
      "   Val MRR: 0.976\n",
      "   ✓ New best model saved! (acc=0.951)\n",
      "\n",
      "\n",
      " Training complete!\n",
      "   Best validation accuracy: 0.951\n",
      "   Model saved to: ../checkpoints/EBM_scorer.ckpt\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    # Freeze encoder for first few epochs (train projection only = faster!)\n",
    "    if epoch < FREEZE_ENCODER_EPOCHS:\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(f\"Epoch {epoch}: Encoder FROZEN (training projection only)\")\n",
    "    else:\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(f\"Epoch {epoch}: Encoder UNFROZEN (full training)\")\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} {'[MNR]' if USE_MNR else '[Triplet]'}\")\n",
    "    \n",
    "    for batch_idx, (enc_q, enc_pos, enc_neg) in enumerate(pbar):\n",
    "        enc_q = {k: v.to(device) for k, v in enc_q.items()}\n",
    "        enc_pos = {k: v.to(device) for k, v in enc_pos.items()}\n",
    "        enc_neg = {k: v.to(device) for k, v in enc_neg.items()}\n",
    "        \n",
    "        batch_size = enc_q['input_ids'].shape[0]\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            emb_q = model(enc_q)\n",
    "            emb_pos = model(enc_pos)\n",
    "            emb_neg = model(enc_neg)\n",
    "            \n",
    "            if USE_MNR:\n",
    "                temperature = 0.05\n",
    "                scores = torch.matmul(emb_q, emb_pos.T) / temperature\n",
    "                labels = torch.arange(batch_size).to(device)\n",
    "                loss = F.cross_entropy(scores, labels)\n",
    "                \n",
    "                # Add explicit negatives\n",
    "                neg_scores = F.cosine_similarity(emb_q, emb_neg) / temperature\n",
    "                pos_scores = scores.diagonal()\n",
    "                margin_loss = F.relu(0.2 - (pos_scores - neg_scores)).mean()\n",
    "                loss = loss + 0.5 * margin_loss\n",
    "            else:\n",
    "                pos_sim = F.cosine_similarity(emb_q, emb_pos)\n",
    "                neg_sim = F.cosine_similarity(emb_q, emb_neg)\n",
    "                loss = torch.clamp(0.5 - (pos_sim - neg_sim), min=0).mean()\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Update every N steps\n",
    "        if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)  # Unscale before step\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()  # Scheduler AFTER optimizer (fixes warning)\n",
    "        \n",
    "        total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f}\",\n",
    "            \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    metrics = {'correct': 0, 'total': 0, 'mrr': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for enc_q, enc_pos, enc_neg in tqdm(val_loader, desc=\"Validation\"):\n",
    "            enc_q = {k: v.to(device) for k, v in enc_q.items()}\n",
    "            enc_pos = {k: v.to(device) for k, v in enc_pos.items()}\n",
    "            enc_neg = {k: v.to(device) for k, v in enc_neg.items()}\n",
    "            \n",
    "            with torch.amp.autocast('cuda'):\n",
    "                emb_q = model(enc_q)\n",
    "                emb_pos = model(enc_pos)\n",
    "                emb_neg = model(enc_neg)\n",
    "            \n",
    "            batch_size = emb_q.shape[0]\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                q_emb = emb_q[i:i+1]\n",
    "                candidates = torch.cat([emb_pos[i:i+1], emb_neg[i:i+1]], dim=0)\n",
    "                sims = F.cosine_similarity(q_emb, candidates, dim=1)\n",
    "                \n",
    "                rank = 2 - (sims[0] > sims[1]).long().item()\n",
    "                \n",
    "                if rank == 1:\n",
    "                    metrics['correct'] += 1\n",
    "                \n",
    "                metrics['mrr'].append(1.0 / rank)\n",
    "                metrics['total'] += 1\n",
    "    \n",
    "    val_acc = metrics['correct'] / metrics['total']\n",
    "    val_mrr = sum(metrics['mrr']) / len(metrics['mrr'])\n",
    "    \n",
    "    print(f\"\\n Epoch {epoch} Summary:\")\n",
    "    print(f\"   Train Loss: {avg_loss:.4f}\")\n",
    "    print(f\"   Val Accuracy: {val_acc:.3f}\")\n",
    "    print(f\"   Val MRR: {val_mrr:.3f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        no_improve_count = 0\n",
    "        torch.save(model.state_dict(), \"../checkpoints/best_EBM_scorer.ckpt\")\n",
    "        print(f\"   ✓ New best model saved! (acc={val_acc:.3f})\")\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        print(f\"   ⚠ No improvement for {no_improve_count} epoch(s)\")\n",
    "        \n",
    "        if no_improve_count >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"\\n⏹ Early stopping! No improvement for {EARLY_STOP_PATIENCE} epochs\")\n",
    "            break\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"\\n✅ Training complete!\")\n",
    "print(f\"   Best validation accuracy: {best_val_acc:.3f}\")\n",
    "print(f\"   Best model saved at: ../checkpoints/best_EBM_scorer.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77bbd66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 [MNR]:   0%|          | 44/9049 [00:10<35:27,  4.23it/s, loss=0.2191] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m         loss = torch.clamp(\u001b[32m0.5\u001b[39m - (pos_sim - neg_sim), \u001b[38;5;28mmin\u001b[39m=\u001b[32m0\u001b[39m).mean()\n\u001b[32m     49\u001b[39m opt.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m scaler.step(opt)\n\u001b[32m     52\u001b[39m scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VIDUSHI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VIDUSHI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VIDUSHI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train with MNR Loss (recommended!)\n",
    "USE_MNR = True  # Set to False for original triplet loss\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "n = 5  # epochs\n",
    "\n",
    "for epoch in range(n):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} {'[MNR]' if USE_MNR else '[Triplet]'}\")\n",
    "    \n",
    "    for enc_q, enc_pos, enc_neg in pbar:\n",
    "        enc_q = {k: v.to(device) for k, v in enc_q.items()}\n",
    "        enc_pos = {k: v.to(device) for k, v in enc_pos.items()}\n",
    "        enc_neg = {k: v.to(device) for k, v in enc_neg.items()}\n",
    "        \n",
    "        batch_size = enc_q['input_ids'].shape[0]\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            emb_q = model(enc_q)       # [B, D]\n",
    "            emb_pos = model(enc_pos)   # [B, D]\n",
    "            emb_neg = model(enc_neg)   # [B, D]\n",
    "            \n",
    "            if USE_MNR:\n",
    "                # MNR Loss: use in-batch negatives\n",
    "                temperature = 0.05\n",
    "                \n",
    "                # In-batch negatives: all positives as negatives for other queries\n",
    "                scores = torch.matmul(emb_q, emb_pos.T) / temperature  # [B, B]\n",
    "                \n",
    "                # Labels: diagonal elements are the positives\n",
    "                labels = torch.arange(batch_size).to(device)\n",
    "                \n",
    "                # Cross-entropy: model should rank correct positive highest\n",
    "                loss = F.cross_entropy(scores, labels)\n",
    "                \n",
    "                # Add explicit hard negatives\n",
    "                neg_scores = F.cosine_similarity(emb_q, emb_neg) / temperature\n",
    "                pos_scores = scores.diagonal()\n",
    "                \n",
    "                # Margin loss between positive and explicit negative\n",
    "                margin_loss = F.relu(0.2 - (pos_scores - neg_scores)).mean()\n",
    "                loss = loss + 0.5 * margin_loss\n",
    "            else:\n",
    "                # Original Triplet Loss\n",
    "                pos_sim = F.cosine_similarity(emb_q, emb_pos)\n",
    "                neg_sim = F.cosine_similarity(emb_q, emb_neg)\n",
    "                loss = torch.clamp(0.5 - (pos_sim - neg_sim), min=0).mean()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # Comprehensive validation\n",
    "    model.eval()\n",
    "    metrics = {'correct': 0, 'total': 0, 'mrr': [], 'recall@5': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for enc_q, enc_pos, enc_neg in tqdm(val_loader, desc=\"Validation\"):\n",
    "            enc_q = {k: v.to(device) for k, v in enc_q.items()}\n",
    "            enc_pos = {k: v.to(device) for k, v in enc_pos.items()}\n",
    "            enc_neg = {k: v.to(device) for k, v in enc_neg.items()}\n",
    "            \n",
    "            with torch.amp.autocast('cuda'):\n",
    "                emb_q = model(enc_q)\n",
    "                emb_pos = model(enc_pos)\n",
    "                emb_neg = model(enc_neg)\n",
    "            \n",
    "            batch_size = emb_q.shape[0]\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                q_emb = emb_q[i:i+1]\n",
    "                candidates = torch.cat([emb_pos[i:i+1], emb_neg[i:i+1]], dim=0)\n",
    "                sims = F.cosine_similarity(q_emb, candidates, dim=1)\n",
    "                \n",
    "                # Rank: 1 if positive > negative, 2 otherwise\n",
    "                rank = 2 - (sims[0] > sims[1]).long().item()\n",
    "                \n",
    "                if rank == 1:\n",
    "                    metrics['correct'] += 1\n",
    "                \n",
    "                metrics['mrr'].append(1.0 / rank)\n",
    "                metrics['recall@5'].append(1.0 if rank <= 5 else 0.0)\n",
    "                metrics['total'] += 1\n",
    "    \n",
    "    # Print comprehensive metrics\n",
    "    accuracy = metrics['correct'] / metrics['total'] if metrics['total'] > 0 else 0\n",
    "    mrr = sum(metrics['mrr']) / len(metrics['mrr']) if metrics['mrr'] else 0\n",
    "    recall5 = sum(metrics['recall@5']) / len(metrics['recall@5']) if metrics['recall@5'] else 0\n",
    "    \n",
    "    print(f\"Epoch {epoch}: accuracy={accuracy:.3f}, MRR={mrr:.3f}, recall@5={recall5:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b730f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"EBM_scorer.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1a19bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Query:\n",
      "Which of the following is not a usual feature of right middle cerebral aery territory infarct?\n",
      "\n",
      "======================================================================\n",
      "Ranked passages (ascending energy = best match):\n",
      "======================================================================\n",
      "\n",
      "1. ✓ HIGH ✓ GOLD\n",
      "   Similarity: 0.7577 | Energy: 0.2423\n",
      "   The coical branches of the MCA supply the lateral surface of the hemisphere except for (1) the frontal pole and a strip along the superomedial border of the frontal and parietal lobes supplied by the ACA, and (2) the low...\n",
      "\n",
      "2. ✓ HIGH\n",
      "   Similarity: 0.7155 | Energy: 0.2845\n",
      "   Basilar aery (Ref: B.D. Chaurasia, 3rd Ed, Vol lll/Pg 300) Basilar aery & its branches: The basilar aery is formed by the union of the right & left veebral aeries, at the lower border of pons. It ascends in the midline, ...\n",
      "\n",
      "3. ⚠ MEDIUM\n",
      "   Similarity: 0.5047 | Energy: 0.4953\n",
      "   The symptoms in this question are suggestive of nihilistic delusions (intestine are rotting away) and pathological guilt (belief that patient is responsible for death). Both these symptoms are usually seen in patients wi...\n",
      "\n",
      "======================================================================\n",
      "Energy Score Guide:\n",
      "  Energy ≈ 0.0-0.3  → ✓ HIGH confidence (good match)\n",
      "  Energy ≈ 0.3-0.5  → ⚠ MEDIUM confidence (acceptable)\n",
      "  Energy ≈ 0.5+     → ✗ LOW confidence (poor match)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Ranking inference: compute energy scores\n",
    "def rank_passages_with_energy(model, tokenizer, query, passages, device=\"cuda\", max_len=128, batch_size=32):\n",
    "    \"\"\"Rank passages and compute energy scores.\n",
    "    \n",
    "    Energy = 1 - similarity (lower energy = better match, closer to 0)\n",
    "    Higher similarity = passage more relevant to query.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    similarity_scores = []\n",
    "    \n",
    "    autocast_device = \"cuda\" if str(device).startswith(\"cuda\") and torch.cuda.is_available() else \"cpu\"\n",
    "    with torch.no_grad(), torch.amp.autocast(autocast_device):\n",
    "        # Encode query separately\n",
    "        enc_query = tokenizer(\n",
    "            query,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        enc_query = {k: v.to(device) for k, v in enc_query.items()}\n",
    "        query_embedding = model(enc_query)  # [1, embedding_dim]\n",
    "        \n",
    "        # Score each passage\n",
    "        for i in range(0, len(passages), batch_size):\n",
    "            batch = passages[i : i + batch_size]\n",
    "            enc = tokenizer(\n",
    "                batch,  # Encode passages only\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            passage_embeddings = model(enc)  # [batch_size, embedding_dim]\n",
    "            \n",
    "            # Cosine similarity between query and each passage\n",
    "            sims = F.cosine_similarity(query_embedding, passage_embeddings)  # [batch_size]\n",
    "            similarity_scores.extend(sims.detach().cpu().tolist())\n",
    "    \n",
    "    # Convert similarity to energy: energy = 1 - similarity\n",
    "    # Lower energy (closer to 0) = better match\n",
    "    energy_scores = [1 - sim for sim in similarity_scores]\n",
    "    \n",
    "    # Sort by energy (ascending = best first, lowest energy = highest confidence)\n",
    "    ranked = sorted(zip(passages, similarity_scores, energy_scores), key=lambda x: x[2])\n",
    "    return ranked\n",
    "\n",
    "\n",
    "# Test on a random positive example\n",
    "source_pos = positives_clean if 'positives_clean' in globals() else [(q, p) for (q, p) in positives if q and p]\n",
    "q, gold = random.choice(source_pos)\n",
    "\n",
    "# Find easy/hard negatives for the same question\n",
    "easy_p = None\n",
    "hard_p = None\n",
    "\n",
    "if 'easy_negatives' in globals() and easy_negatives:\n",
    "    same_q_easy = [p for (qq, p) in easy_negatives if qq == q and p]\n",
    "    if same_q_easy:\n",
    "        easy_p = random.choice(same_q_easy)\n",
    "    else:\n",
    "        pool = [p for (_, p) in easy_negatives if p and p != gold]\n",
    "        easy_p = random.choice(pool) if pool else None\n",
    "\n",
    "if 'hard_negatives' in globals() and hard_negatives:\n",
    "    same_q_hard = [p for (qq, p) in hard_negatives if qq == q and p]\n",
    "    if same_q_hard:\n",
    "        hard_p = random.choice(same_q_hard)\n",
    "    else:\n",
    "        pool = [p for (_, p) in hard_negatives if p and p != gold]\n",
    "        hard_p = random.choice(pool) if pool else None\n",
    "\n",
    "# Build passages to rank\n",
    "test_passages = [gold]\n",
    "if easy_p: test_passages.append(easy_p)\n",
    "if hard_p: test_passages.append(hard_p)\n",
    "\n",
    "# Rank them with energy scores\n",
    "ranked = rank_passages_with_energy(model, tokenizer, q, test_passages, device=device, max_len=128, batch_size=32)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Query:\")\n",
    "print(q[:300])\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Ranked passages (ascending energy = best match):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (passage, similarity, energy) in enumerate(ranked, 1):\n",
    "    snippet = (passage or \"\").replace(\"\\n\", \" \")[:220]\n",
    "    is_gold = \" ✓ GOLD\" if passage == gold else \"\"\n",
    "    \n",
    "    # Color-code energy: green if low (good), red if high (bad)\n",
    "    confidence = \"✓ HIGH\" if energy < 0.3 else \"⚠ MEDIUM\" if energy < 0.5 else \"✗ LOW\"\n",
    "    \n",
    "    print(f\"\\n{i}. {confidence}{is_gold}\")\n",
    "    print(f\"   Similarity: {similarity:.4f} | Energy: {energy:.4f}\")\n",
    "    print(f\"   {snippet}{'...' if passage and len(passage) > 220 else ''}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Energy Score Guide:\")\n",
    "print(\"  Energy ≈ 0.0-0.3  → ✓ HIGH confidence (good match)\")\n",
    "print(\"  Energy ≈ 0.3-0.5  → ⚠ MEDIUM confidence (acceptable)\")\n",
    "print(\"  Energy ≈ 0.5+     → ✗ LOW confidence (poor match)\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
